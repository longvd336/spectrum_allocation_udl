{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a84e5f29",
   "metadata": {},
   "source": [
    "## This repo purpose is reconstruct the paper named: \"An Unsupervised Learning approach for spectrum allocation in Terahertz communication systems\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547132b8",
   "metadata": {},
   "source": [
    "## 1. Parameter & System model.\n",
    "\n",
    "### 1.1. System model: \n",
    "- 3D indoor ThzCom system support *nI* users with 1 Access Point\n",
    "- User distributed uniformly on the floor \n",
    "- Vector d: *nI* x 1 vector represent for the distance vector btw user with AP. The element in d are ordered such that d1 < d2 < ... < dnI\n",
    "\n",
    "#### A. Spectrum of interest:\n",
    "- Divide the ultra-wide band THz transmission window into 2 areas callded NACSR & PACSR. This paper experiment is on the NACSR\n",
    "- Focus on multiband based spectrum allocation with ASB \n",
    "- Spectrum interested is divided into *nS* sub-bands with unequal bw\n",
    "- *b* & *f* as the *nSx1* vectors of the BW and the center frequency of the sub-bands.\n",
    "\n",
    "#### B. Archievable Data Rate\n",
    "- *r* as the *nSx1* rate vectore of users.\n",
    "- The rate achieve in the *s*th subband is calculated through the formula (4) in paper.\n",
    "\n",
    "#### C. Optimal Spectrum Allocation:\n",
    "- Consider proportionally-fair data rate maximization\n",
    "- Total data rate = (1xnS)T x log(r)\n",
    "- Hard to optimize due to the data rate formular rely on parameter b and the molecular absorbption coefficient at f\n",
    "\n",
    "#### D. DNN architecture:\n",
    "- 5 layers: 100, 100, 50, 25, 30 neural:\n",
    "- Activation: ReLU for 4 layers, sigmoid for last layer\n",
    "- Initial weight: Gaussian random variables with zero mean and unit variance\n",
    "- Initial biases are set to 0.\n",
    "- The initial values of Î» are set to a small constant of 0.1\n",
    "- Num interation: 500\n",
    "- Number of realization of d, nT: 300\n",
    "- Learning rate: 0.05 for weight, 0.025 for largrange coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54a6b2e",
   "metadata": {},
   "source": [
    "## 1. Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b43d5d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# gen distance vector d for 15 user\n",
    "# 1 batch: 300 vector d\n",
    "# intergration: 500\n",
    "# => 500*300 vector d\n",
    "def gen_data():\n",
    "    data = np.random.uniform((0, 0), (25, 25), (15, 2))\n",
    "    x_y_ap = [12.5, 12.5]\n",
    "    # calculate vector d\n",
    "    _d = np.abs(x_y_ap - data)\n",
    "    _d = pow(_d,2)\n",
    "    d = _d[:,0] + _d[:,1]\n",
    "    \n",
    "    d = np.sqrt(d + pow(1.7,2))\n",
    "    return d\n",
    "X = []\n",
    "for i in range(300): \n",
    "    d = gen_data()\n",
    "    d.sort()\n",
    "    X.append(d)\n",
    "X = np.array(X)\n",
    "X = X.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be3fa607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 300)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3017735",
   "metadata": {},
   "source": [
    "## 2. Deep learning model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6edb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate data rate & some coefficients\n",
    "\n",
    "import scipy.integrate as integrate\n",
    "import math\n",
    "\n",
    "def molecular_absorption():\n",
    "    \n",
    "\n",
    "def lamda():\n",
    "    ld = ga*gu*(1/No)*pow((3*pow(10,8))/(4*math.pi),2)\n",
    "\n",
    "def data_rate(p, b, d):\n",
    "    return math.log((1 + (p*lamda*np.exp(-molecular_absorption*d))/(pow(x, 2)*pow(d,2)*b)),2)\n",
    "    \n",
    "upper_bound = f + b / 2\n",
    "lower_bound = f - b / 2\n",
    "result = integrate.quad(data_rate, lower_bound, upper_bound)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b510176f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convex Optimization\n",
    "# NACSR \n",
    "# => srn1: 0.557 - 0.671 THz\n",
    "# => srn2: 0.752 - 0.868 THz\n",
    "# Approximate frequency:\n",
    "import scipy.integrate as integrate\n",
    "import scipy.special as special\n",
    "import math\n",
    "\n",
    "def approx_molecular_absorp_1(f):\n",
    "    #for snr2\n",
    "    n1 = 10**0.83\n",
    "    n2 = -10**(-10.04)\n",
    "    n3 = -10**(-1.23)\n",
    "    sum_ = n1 + n2*f\n",
    "    k = (math.e)**sum_ + n3\n",
    "    return k\n",
    "\n",
    "def approx_molecular_absorp_2(f):\n",
    "    #for snr2\n",
    "    n1 = 10**(0.89)\n",
    "    n2 = -10**(-10.8)\n",
    "    n3 = -10**(-1.53)\n",
    "    sum_ = n1 + n2*f\n",
    "    k = (math.e)**sum_ + n3\n",
    "    return k\n",
    "\n",
    "def bw_to_f(b,bs,fd):\n",
    "    result = list(b).index(bs)\n",
    "    fs = fd + sum(b[0:(result-1)]) + bs/2\n",
    "    return fs\n",
    "\n",
    "def data_rate(fd, b, bs, ps, ds, Ga, Gu, No, kf):\n",
    "    fs = bw_to_f(b, bs, fd) # calculate the central frequency of sub-band \n",
    "    gamma = Ga * Gu * (1/No) * (((3 * (10**8))/(4*math.pi))**2)\n",
    "    func = lambda x: math.log((1 + (ps * gamma * ((math.e)**(-kf * ds)))/((x**2)*(ds**2)*bs)),2)\n",
    "    result = integrate.quad(func , fs - 0.5*bs, fs + 0.5*bs)\n",
    "    return result\n",
    "\n",
    "# def total_data_rate(d, p_pred, b_pred):\n",
    "def loss_function(total_data_rate, p_pred, b_pred, Ptot, btot, lamda1, lamda2):\n",
    "    loss = tf.reduce_mean(-total_data_rate + lamda1*(p_pred - Ptot) + lamda2*(b_pred - btot))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "38bf98a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226.8938797705266"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = Ga * Gu * (1/No) * (((3 * (10**8))/(4*math.pi))**2)\n",
    "gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e7c37c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 300)\n",
      "(15, 300)\n",
      "(15, 300)\n",
      "[[2.50000596 2.50000641 2.50000574 ... 2.50000606 2.5000059  2.50000574]\n",
      " [2.50000173 2.50000184 2.50000164 ... 2.50000164 2.50000181 2.50000157]\n",
      " [2.49999127 2.49999122 2.49999083 ... 2.49999153 2.49999065 2.49999149]\n",
      " ...\n",
      " [2.49999829 2.49999887 2.49999825 ... 2.49999889 2.49999818 2.49999881]\n",
      " [2.50000218 2.50000254 2.50000233 ... 2.50000262 2.50000212 2.50000261]\n",
      " [2.49999606 2.49999594 2.49999591 ... 2.49999611 2.49999563 2.49999594]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Multiple layer\n",
    "d0 = 15 #input\n",
    "d1 = 100 # 1st layer\n",
    "d2 = 100 # 2nd layer\n",
    "d3 = 50 # 3rd layer\n",
    "d4 = 25 #4th layer\n",
    "d5 = 30 #output layer\n",
    "\n",
    "# System params\n",
    "N = X.shape # batch size\n",
    "height = 1.7\n",
    "Ga = 10**3 #30 dbi\n",
    "Gu = 10**2 #20 dbi\n",
    "No = 10**(17.4) #dbm/hz\n",
    "Ptot = 0.0003 #-5dbm\n",
    "pmax = (5/4)*(Ptot/15)\n",
    "bmax = 5 #Ghz\n",
    "fd = 752 #Ghz\n",
    "\n",
    "\n",
    "# hyper params for Unsuppervised model\n",
    "learning_rate_weight_bias = 0.05\n",
    "learning_rate_lagrange = 0.025\n",
    "\n",
    "# 5 layers\n",
    "# initial parameters randomly\n",
    "W1 = 0.01*np.random.normal(size=(d0,d1))\n",
    "b1 = np.zeros((d1,1))\n",
    "W2 = 0.01*np.random.normal(size=(d1,d2))\n",
    "b2 = np.zeros((d2,1))\n",
    "W3 = 0.01*np.random.normal(size=(d2,d3))\n",
    "b3 = np.zeros((d3,1))\n",
    "W4 = 0.01*np.random.normal(size=(d3,d4))\n",
    "b4 = np.zeros((d4,1))\n",
    "W5 = 0.01*np.random.normal(size=(d4,d5))\n",
    "b5 = np.zeros((d5,1))\n",
    "\n",
    "def relu_derivative(x):\n",
    "    x[x <= 0] = 0\n",
    "    x[x > 0] = 1\n",
    "    return x\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "## loop via all data\n",
    "# batch_size = 300\n",
    "# interation = 500\n",
    "def sig(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "# reshape(X)\n",
    "\n",
    "for i in range(1):\n",
    "    ## Feed forward\n",
    "    Z1 = np.dot(W1.T, X) + b1\n",
    "    A1 = np.maximum(Z1, 0) #ReLU\n",
    "    Z2 = np.dot(W2.T, A1) + b2\n",
    "    A2 = np.maximum(Z2, 0) #ReLU\n",
    "    Z3 = np.dot(W3.T, A2) + b3\n",
    "    A3 = np.maximum(Z3, 0) #ReLU\n",
    "    Z4 = np.dot(W4.T, A3) + b4\n",
    "    A4 = np.maximum(Z4, 0) #ReLU\n",
    "    Z5 = np.dot(W5.T, A4) + b5\n",
    "    Y = sig(Z5)\n",
    "    print(Y.shape)\n",
    "    \n",
    "    ## We need to multiply the result of the backprop with the derivative of Loss function with p and b predicted.\n",
    "    ## dL/d0 = dY/d0*dL/dY\n",
    "    \n",
    "    p_pred = Y[:15,:] * pmax\n",
    "    print(p_pred.shape)\n",
    "    b_pred = Y[15:,:] * bmax\n",
    "    print(b_pred.shape)\n",
    "    \n",
    "    eps = 1e-4\n",
    "    # Derive with p => Calculated through definition of derivative\n",
    "    p_eps = p_pred + p_pred*eps\n",
    "    b_eps = b_pred + b_pred*eps\n",
    "    \n",
    "    dL_y = np.zeros(Y.shape)\n",
    "    data_rate_dp_b = np.zeros(Y.shape)\n",
    "    data_rate_dp_p = np.zeros(Y.shape)\n",
    "    \n",
    "    print(b_pred)\n",
    "    \n",
    "    for i in range(0,1):\n",
    "#     for i in range(0,300):\n",
    "        inp = X[:, i]\n",
    "        #not chnage\n",
    "        dpb = np.stack((inp, p_pred[:, i], b_pred[:, i]), axis = 1)\n",
    "        #p change\n",
    "        dp_b = np.stack((inp, p_eps[:, i], b_pred[:,i]), axis = 1)\n",
    "        #b change\n",
    "        dpb_ = np.stack((inp, p_pred[:, i], b_eps[:, i]),axis = 1)\n",
    "        ## Can be convert to function => reduce complex\n",
    "        ## Derivative of data rate with p\n",
    "        for j in range(0, 15):\n",
    "            ## Data rate 1\n",
    "            d = dp_b[j, 0]\n",
    "            p = dp_b[j, 1]\n",
    "            b = dp_b[j, 2]\n",
    "            f = bw_to_f(dp_b[:,2],b,fd)\n",
    "            kf = approx_molecular_absorp_2(f)\n",
    "            data_rate_with_d_j = data_rate(fd, dp_b[:,2], b, p, d, Ga, Gu, No, kf)\n",
    "#             print(d, p, b)\n",
    "#             print(dp_b[:,2])\n",
    "#             print(f)\n",
    "#             print(kf)\n",
    "#             print(data_rate_with_d_j)\n",
    "            \n",
    "            \n",
    "#             ## Data rate 2\n",
    "#             d2 = dpb[j, 0]\n",
    "#             p2 = dpb[j, 1]\n",
    "#             b2 = dpb[j, 2]\n",
    "#             f2 = bw_to_f(dpb[:,2],b2,fd)\n",
    "#             kf2 = approx_molecular_absorp_2(f2)\n",
    "#             data_rate_with_d_j_2 = data_rate(fd, dpb[:,2], b2, p2, d2, Ga, Gu, No, kf2)\n",
    "            \n",
    "#             delta_dr1 = (data_rate_with_d_j - data_rate_with_d_j_2) / (p2*eps)\n",
    "#             data_rate_dp_p[i,j] = delta_dr1\n",
    "        \n",
    "        ## Derivative of data rate with b\n",
    "#         for j in range(0, 15):\n",
    "#             ## Data rate 1\n",
    "#             d = dpb_[j, 0]\n",
    "#             p = dpb_[j, 1]\n",
    "#             b = dpb_[j, 2]\n",
    "#             f = bw_to_f(dpb_[:,2],b,fd)\n",
    "#             kf = approx_molecular_absorp_2(f)\n",
    "#             data_rate_with_d_j = data_rate(fd, dpb_[:,2], b, p, d, Ga, Gu, No, kf)\n",
    "            \n",
    "#             ## Data rate 2\n",
    "#             d2 = dpb[j, 0]\n",
    "#             p2 = dpb[j, 1]\n",
    "#             b2 = dpb[j, 2]\n",
    "#             f2 = bw_to_f(dpb[:,2],b2,fd)\n",
    "#             kf2 = approx_molecular_absorp_2(f2)\n",
    "#             data_rate_with_d_j_2 = data_rate(fd, dpb[:,2], b2, p2, d2, Ga, Gu, No, kf2)\n",
    "            \n",
    "#             delta_dr2 = (data_rate_with_d_j - data_rate_with_d_j_2) / (b2*eps)\n",
    "#             data_rate_dp_b[i,j] = delta_dr2\n",
    "        \n",
    "        \n",
    "    ### Get loss function for each interations\n",
    "    \n",
    "    ### average cost\n",
    "#     loss = loss_func(Y, lamda1, lamda2)\n",
    "    \n",
    "    ## Donot need back propagation\n",
    "    ## we can calculate via chain rule\n",
    "    ## train in batch, based on the cost function of this paper => we almost can not initial automation training\n",
    "    ## so i built a neural network from scratch\n",
    "    ## Backpropagation\n",
    "    ## DLoss via weights and bias\n",
    "    ## dL/d0 = dY/d0*dL/dY\n",
    "    ## Via chain rule we can calculate\n",
    "    ## dY/dW5 = dY/dZ5*dZ5/dW5\n",
    "    \n",
    "    ##Backprop 1\n",
    "#     E5 = sigmoid_derivative(Y)\n",
    "#     dw5 = np.dot(A4, E5.T)\n",
    "#     db5 = np.sum(E5, axis = 1, keepdims = True)\n",
    "\n",
    "#     print(\"Backprop1\")\n",
    "#     print(E5.shape)\n",
    "#     print(dw5.shape)\n",
    "#     print(db5.shape)\n",
    "    \n",
    "# #     ## Backprop 2\n",
    "#     E4 = np.dot(W5, E5) * relu_derivative(Z4)\n",
    "#     dw4 = np.dot(A3, E4.T)\n",
    "#     db4 = np.sum(E4, axis = 1, keepdims = True)\n",
    "#     print(\"Backprop2\")\n",
    "#     print(E4.shape)\n",
    "#     print(dw4.shape)\n",
    "#     print(db4.shape)\n",
    "\n",
    "# #     ##Backprop 3\n",
    "#     E3 = np.dot(W4, E4) * relu_derivative(Z3)\n",
    "#     dw3 = np.dot(A2, E3.T)\n",
    "#     db3 = np.sum(E3, axis = 1, keepdims = True)\n",
    "#     print(\"Backprop3\")\n",
    "#     print(E3.shape)\n",
    "#     print(dw3.shape)\n",
    "#     print(db3.shape)\n",
    "    \n",
    "# #     ##Backprop 4\n",
    "#     E2 = np.dot(W3, E3) * relu_derivative(Z2)\n",
    "#     dw2 = np.dot(A1, E2.T)\n",
    "#     db2 = np.sum(E2, axis = 1, keepdims = True)\n",
    "#     print(\"Backprop4\")\n",
    "#     print(E2.shape)\n",
    "#     print(dw2.shape)\n",
    "#     print(db2.shape)\n",
    "    \n",
    "# #     ##Backprop 5 (input)\n",
    "#     E1 = np.dot(W2, E2) * relu_derivative(Z1)\n",
    "#     dw1 = np.dot(X, E1.T)\n",
    "#     db1 = np.sum(E1, axis = 1, keepdims = True)\n",
    "#     print(\"Backprop5\")\n",
    "#     print(E1.shape)\n",
    "#     print(dw1.shape)\n",
    "#     print(db1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28e281a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
