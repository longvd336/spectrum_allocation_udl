{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a84e5f29",
   "metadata": {},
   "source": [
    "## This repo purpose is reconstruct the paper named: \"An Unsupervised Learning approach for spectrum allocation in Terahertz communication systems\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547132b8",
   "metadata": {},
   "source": [
    "## 1. Parameter & System model.\n",
    "\n",
    "### 1.1. System model: \n",
    "- 3D indoor ThzCom system support *nI* users with 1 Access Point\n",
    "- User distributed uniformly on the floor \n",
    "- Vector d: *nI* x 1 vector represent for the distance vector btw user with AP. The element in d are ordered such that d1 < d2 < ... < dnI\n",
    "\n",
    "#### A. Spectrum of interest:\n",
    "- Divide the ultra-wide band THz transmission window into 2 areas callded NACSR & PACSR. This paper experiment is on the NACSR\n",
    "- Focus on multiband based spectrum allocation with ASB \n",
    "- Spectrum interested is divided into *nS* sub-bands with unequal bw\n",
    "- *b* & *f* as the *nSx1* vectors of the BW and the center frequency of the sub-bands.\n",
    "\n",
    "#### B. Archievable Data Rate\n",
    "- *r* as the *nSx1* rate vectore of users.\n",
    "- The rate achieve in the *s*th subband is calculated through the formula (4) in paper.\n",
    "\n",
    "#### C. Optimal Spectrum Allocation:\n",
    "- Consider proportionally-fair data rate maximization\n",
    "- Total data rate = (1xnS)T x log(r)\n",
    "- Hard to optimize due to the data rate formular rely on parameter b and the molecular absorbption coefficient at f\n",
    "\n",
    "#### D. DNN architecture:\n",
    "- 5 layers: 100, 100, 50, 25, 30 neural:\n",
    "- Activation: ReLU for 4 layers, sigmoid for last layer\n",
    "- Initial weight: Gaussian random variables with zero mean and unit variance\n",
    "- Initial biases are set to 0.\n",
    "- The initial values of Î» are set to a small constant of 0.1\n",
    "- Num interation: 500\n",
    "- Number of realization of d, nT: 300\n",
    "- Learning rate: 0.05 for weight, 0.025 for largrange coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3017735",
   "metadata": {},
   "source": [
    "## 2. Deep learning model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3217b8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_5 (Dense)             (None, 100)               1600      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 50)                5050      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 25)                1275      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 30)                780       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,805\n",
      "Trainable params: 18,805\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_shape=(15,), activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(30, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "# compile the keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6edb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate data rate & some coefficients\n",
    "\n",
    "import scipy.integrate as integrate\n",
    "import math\n",
    "\n",
    "def molecular_absorption():\n",
    "    \n",
    "\n",
    "def lamda():\n",
    "    ld = ga*gu*(1/No)*pow((3*pow(10,8))/(4*math.pi),2)\n",
    "\n",
    "def data_rate(p, b, d):\n",
    "    return math.log((1 + (p*lamda*np.exp(-molecular_absorption*d))/(pow(x, 2)*pow(d,2)*b)),2)\n",
    "    \n",
    "upper_bound = f + b / 2\n",
    "lower_bound = f - b / 2\n",
    "result = integrate.quad(data_rate, lower_bound, upper_bound)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b43d5d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.05811423,  4.06855733,  4.70903136, ..., 13.07754905,\n",
       "        14.21658381, 14.47973583],\n",
       "       [ 2.22508973,  3.24851684,  8.76046136, ..., 12.42967966,\n",
       "        13.18480443, 16.36421973],\n",
       "       [ 3.42136677,  6.43581289,  7.82385273, ..., 13.74271586,\n",
       "        13.87259477, 14.12983574],\n",
       "       ...,\n",
       "       [ 3.66202287,  4.52656619,  5.16058258, ..., 12.60839335,\n",
       "        12.81131376, 12.82657037],\n",
       "       [ 4.89802499,  6.86406541,  7.59143613, ..., 13.75942238,\n",
       "        13.90441315, 14.16340515],\n",
       "       [ 3.96161699,  5.52494501,  6.02833185, ..., 10.37120238,\n",
       "        10.3764234 , 11.47092841]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# gen distance vector d for 15 user\n",
    "# 1 batch: 300 vector d\n",
    "# intergration: 500\n",
    "# => 500*300 vector d\n",
    "def gen_data():\n",
    "    data = np.random.uniform((0, 0), (25, 25), (15, 2))\n",
    "    x_y_ap = [12.5, 12.5]\n",
    "    # calculate vector d\n",
    "    _d = np.abs(x_y_ap - data)\n",
    "    _d = pow(_d,2)\n",
    "    d = _d[:,0] + _d[:,1]\n",
    "    \n",
    "    d = np.sqrt(d + pow(1.7,2))\n",
    "    return d\n",
    "X = []\n",
    "for i in range(300): \n",
    "    d = gen_data()\n",
    "    d.sort()\n",
    "    X.append(d)\n",
    "X = np.array(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b510176f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convex Optimization\n",
    "# NACSR \n",
    "# => srn1: 0.557 - 0.671 THz\n",
    "# => srn2: 0.752 - 0.868 THz\n",
    "# Approximate frequency:\n",
    "import scipy.integrate as integrate\n",
    "import scipy.special as special\n",
    "import math\n",
    "\n",
    "def approx_molecular_absorp_1(f):\n",
    "    #for snr2\n",
    "    n1 = pow(10, 0.83)\n",
    "    n2 = pow(-10, -10.04)\n",
    "    n3 = pow(-10, -1.23)\n",
    "    k = np.exp(n1 + n2*f) + n3\n",
    "    return k\n",
    "\n",
    "def approx_molecular_absorp_2(f):\n",
    "    #for snr2\n",
    "    n1 = pow(10, 0.89)\n",
    "    n2 = pow(-10, -10.8)\n",
    "    n3 = pow(-10, -1.53)\n",
    "    k = np.exp(n1 + n2*f) + n3\n",
    "    return k\n",
    "\n",
    "def bw_to_f(b,bs,fd):\n",
    "    fs = fd + sum(b[:b.index(bs)]) + bs/2\n",
    "    return fs\n",
    "\n",
    "def data_rate(fd, b, bs, ps, ds, Ga, Gu, No, kf):\n",
    "    fs = bw_to_f(b, bs, fd) # calculate the central frequency of sub-band \n",
    "    gamma = Ga * Gu * (1/No) * (((3 * pow(10, 8))/(4*math.pi))**2)\n",
    "    func = lambda x: math.log((1 + (ps * gamma * math.exp(-kf * ds))/(pow(x, 2)*pow(d, 2)*ds)),2)\n",
    "    result = integrate.quad(func , fs - 0.5*bs, fs + 0.5*bs)\n",
    "    return result\n",
    "\n",
    "def total_data_rate(d, p_pred, b_pred):\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ee23c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 100)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = 0.01*np.random.randn(2, 100)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bb4891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(total_data_rate, p_pred, b_pred, Ptot, btot, lamda1, lamda2):\n",
    "    loss = tf.reduce_mean(-total_data_rate + lamda1*(p_pred - Ptot) + lamda2*(b_pred - btot))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7c37c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Multiple layer\n",
    "d0 = 15 #input\n",
    "d1 = 100 # 1st layer\n",
    "d2 = 100 # 2nd layer\n",
    "d3 = 50 # 3rd layer\n",
    "d4 = 25 #4th layer\n",
    "d5 = 30 #output layer\n",
    "\n",
    "# System params\n",
    "N = X.shape # batch size\n",
    "height = 1.7\n",
    "Ga = 30 #dbi\n",
    "Gu = 20 #dbi\n",
    "No = -174 #dbm/hz\n",
    "Ptot = -5 #dbm\n",
    "pmax = (5/4)*(Ptot/15)\n",
    "bmax = 5 #Ghz\n",
    "\n",
    "# hyper params for Unsuppervised model\n",
    "learning_rate_weight_bias = 0.05\n",
    "learning_rate_lagrange = 0.025\n",
    "\n",
    "# 5 layers\n",
    "# initial parameters randomly\n",
    "W1 = 0.01*np.random.normal(size=(d0,d1))\n",
    "b1 = np.zeros((d1,1))\n",
    "W2 = 0.01*np.random.normal(size=(d1,d2))\n",
    "b2 = np.zeros((d2,1))\n",
    "W3 = 0.01*np.random.normal(size=(d2,d3))\n",
    "b3 = np.zeros((d3,1))\n",
    "W4 = 0.01*np.random.normal(size=(d3,d4))\n",
    "b4 = np.zeros((d4,1))\n",
    "W5 = 0.01*np.random.normal(size=(d4,d5))\n",
    "b5 = np.zeros((d5,1))\n",
    "\n",
    "## loop via all data\n",
    "# batch_size = 300\n",
    "# interation = 500\n",
    "def sig(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "for i in range(500):\n",
    "    ## Feed forward\n",
    "    Z1 = np.dot(W1.T, X) + b1\n",
    "    A1 = np.maximum(Z1, 0)\n",
    "    Z2 = np.dot(W2.T, A1) + b2\n",
    "    A2 = np.maximum(Z2, 0)\n",
    "    Z3 = np.dot(W3.T, A2) + b3\n",
    "    A3 = np.maximum(Z3, 0)\n",
    "    Z4 = np.dot(W4.T, A3) + b4\n",
    "    A4 = np.maximum(Z4, 0)\n",
    "    Z5 = np.dot(W5.T, A4) + b5\n",
    "    Y = sig(Z5)\n",
    "    \n",
    "    ### Get loss function for each interations\n",
    "    ### average cost\n",
    "    loss = loss_func(Y)\n",
    "    \n",
    "    ## Donot need back propagation\n",
    "    ## we can calculate via chain rule\n",
    "    ## train in batch\n",
    "    ## \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba4b62a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def backward_propagation(weights, biases, X, Y, activations, zs):\n",
    "    m = X.shape[0]\n",
    "    delta_w5 = np.zeros(weights[4].shape)\n",
    "    delta_b5 = np.zeros(biases[4].shape)\n",
    "    delta_w4 = np.zeros(weights[3].shape)\n",
    "    delta_b4 = np.zeros(biases[3].shape)\n",
    "    delta_w3 = np.zeros(weights[2].shape)\n",
    "    delta_b3 = np.zeros(biases[2].shape)\n",
    "    delta_w2 = np.zeros(weights[1].shape)\n",
    "    delta_b2 = np.zeros(biases[1].shape)\n",
    "    delta_w1 = np.zeros(weights[0].shape)\n",
    "    delta_b1 = np.zeros(biases[0].shape)\n",
    "\n",
    "    for i in range(m):\n",
    "        a5 = activations[4][i, :].reshape(30, 1)\n",
    "        z4 = zs[3][i, :].reshape(25, 1)\n",
    "        a4 = activations[3][i, :].reshape(25, 1)\n",
    "        z3 = zs[2][i, :].reshape(50, 1)\n",
    "        a3 = activations[2][i, :].reshape(50, 1)\n",
    "        z2 = zs[1][i, :].reshape(100, 1)\n",
    "        a2 = activations[1][i, :].reshape(100, 1)\n",
    "        z1 = zs[0][i, :].reshape(100, 1)\n",
    "        a1 = activations[0][i, :].reshape(100, 1)\n",
    "        x = X[i, :].reshape(400, 1)\n",
    "        y = Y[i, :].reshape(30, 1)\n",
    "\n",
    "        d5 = (a5 - y) * sigmoid_derivative(a5)\n",
    "        delta_w5 += np.dot(d5, a4.T)\n",
    "        delta_b5 += d5\n",
    "\n",
    "        d4 = np.dot(weights[4].T, d5) * sigmoid_derivative(a4)\n",
    "        delta_w4 += np.dot(d4, a3.T)\n",
    "        delta_b4 += d4\n",
    "\n",
    "        d3 = np.dot(weights[3].T, d4) * sigmoid_derivative(a3)\n",
    "        delta_w3 += np.dot(d3, a2.T)\n",
    "        delta_b3 += d3\n",
    "\n",
    "        d2 = np.dot(weights[2].T, d3) * sigmoid_derivative(a2)\n",
    "        delta_w2 += np.dot(d2, a1.T)\n",
    "        delta_b2 += d2\n",
    "\n",
    "        d1 = np.dot(weights[1].T, d2) * sigmoid_derivative(a1)\n",
    "        delta_w1 += np.dot(d1, a1.T)\n",
    "        delta_b1 += d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14fe0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    x[x <= 0] = 0\n",
    "    x[x > 0] = 1\n",
    "    return x\n",
    "\n",
    "def forward_propagation(weights, biases, X):\n",
    "    activations = []\n",
    "    zs = []\n",
    "    a = X\n",
    "    for i in range(len(weights) - 1):\n",
    "    z = np.dot(a, weights[i]) + biases[i]\n",
    "    zs.append(z)\n",
    "    if i == len(weights) - 2:\n",
    "        a = sigmoid(z)\n",
    "    else:\n",
    "        a = relu(z)\n",
    "    activations.append(a)\n",
    "    return activations, zs\n",
    "\n",
    "def backward_propagation(weights, biases, X, activations, zs):\n",
    "    m = X.shape[0]\n",
    "    delta_w = [np.zeros(w.shape) for w in weights]\n",
    "    delta_b = [np.zeros(b.shape) for b in biases]\n",
    "\n",
    "    for i in range(m):\n",
    "    a_last = activations[-1][i, :].reshape(-1, 1)\n",
    "    z_last = zs[-1][i, :].reshape(-1, 1)\n",
    "    \n",
    "    for j in range(len(weights) - 1, 0, -1):\n",
    "        a = activations[j - 1][i, :].reshape(-1, 1)\n",
    "        z = zs[j - 1][i, :].reshape(-1, 1)\n",
    "        \n",
    "        if j == len(weights) - 1:\n",
    "            delta = (a_last - X[i, :].reshape(-1, 1)) * sigmoid_derivative(a_last)\n",
    "        \n",
    "        else:\n",
    "            delta = np.dot(weights[j + 1], delta) * relu_derivative(a)\n",
    "            delta_w[j] += np.dot(delta, a.T)\n",
    "            delta_b[j] += delta\n",
    "            a_last = a\n",
    "        \n",
    "    return delta_w, delta_b\n",
    "\n",
    "def unsupervised_learning(X, n_hidden, n_epochs, learning_rate):\n",
    "    n_input = X.shape[1]\n",
    "    n_output = X.shape[1]\n",
    "    n_neurons = [n_input] + n_hidden + [n_output]\n",
    "    weights = [np.random.normal(0, 1, (n_neurons[i], n_neurons[i + 1])) for i in range(len(n_neurons) - 1)]\n",
    "    biases = [np.zeros((1, n)) for n in n_hidden + [n_output]]\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        activations, zs = forward_propagation(weights, biases, X)\n",
    "        delta_w, delta_b = backward_propagation(weights, biases, X, activations, zs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
